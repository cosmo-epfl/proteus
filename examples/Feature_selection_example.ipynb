{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d10a9a",
   "metadata": {},
   "source": [
    "# Using feature selection to build an efficient potential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a81755",
   "metadata": {},
   "source": [
    "In the previous examples (`MLIP_example` and `zundel_i-PI`) we have seen how to use the tools provided by `librascal` in order to build a working potential.  We have already used a \"sparsification\" technique to select a basis set of environments for the fit, as well as to keep the computational cost of both fitting and evaluating the potential manageable.\n",
    "\n",
    "Now we will introduce a technique to further optimize the computational cost of the model, and it consists of applying the same sparsification technique along the _features_ (columns) of the feature matrix, in addition to the _samples_ (rows).  This technique was introduced in Ref. [1] and has since been used in numerous applications. Feature selection and sparse feature computation are both available directly in librascal, and this example aims to show the user how to apply them in a realistic setting.\n",
    "\n",
    "A detail on the selection algorithm itself: `librascal` uses the FPS and CUR algorithms implemented in [`scikit-cosmo`](https://scikit-cosmo.readthedocs.io/en/latest/selection.html).  You will therefore need to have `scikit-cosmo` installed (`pip install skmatter`) to run this notebook.  Note also that further selection algorithms, notably PCovCUR and PCovFPS, are implemented in `skmatter` but not yet included directly in `librascal`.\n",
    "\n",
    "[1]: G. Imbalzano, A. Anelli, D. Giofr√©, S. Klees, J. Behler, and M. Ceriotti, Automatic Selection of Atomic Fingerprints and Reference Configurations for Machine-Learning Potentials, J. Chem. Phys. 148, 241730 (2018). [doi:10.1063/1.5024611](https://aip.scitation.org/doi/full/10.1063/1.5024611)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d9cdd",
   "metadata": {},
   "source": [
    "Let us begin with the Zundel cation example introduced in the previous notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298ec8b",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee33dff6",
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "import ase\n",
    "from ase.io import read, write\n",
    "from ase.build import make_supercell\n",
    "from ase.visualize import view\n",
    "import numpy as np\n",
    "# If installed -- not essential, though\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    tqdm = (lambda i, **kwargs: i)\n",
    "\n",
    "from time import time\n",
    "# Print some extra information during the selection process\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from rascal.models import Kernel, train_gap_model, compute_KNM\n",
    "from rascal.representations import SphericalInvariants\n",
    "from rascal.utils import from_dict, to_dict, CURFilter, FPSFilter, dump_obj, load_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0320d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd i-PI/zundel/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8da102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first N structures of the zundel dataset\n",
    "N_dataset = 1000\n",
    "frames = read('zundel_dataset.xyz', index=':{}'.format(N_dataset))\n",
    "energies = np.loadtxt('zundel_energies.txt')[:N_dataset]\n",
    "forces = np.concatenate([frame.arrays['forces'] for frame in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d198335",
   "metadata": {},
   "outputs": [],
   "source": [
    "forces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea5828",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Number of structures to train the model with\n",
    "n_train = 800\n",
    "\n",
    "global_species = [1, 8]\n",
    "\n",
    "# Select randomly n structures for training the model\n",
    "ids = list(range(N_dataset))\n",
    "np.random.seed(10)\n",
    "np.random.shuffle(ids)\n",
    "\n",
    "train_ids = ids[:n_train]\n",
    "frames_train = [frames[ii] for ii in ids[:n_train]]\n",
    "e_train = np.array([energies[ii] for ii in ids[:n_train]])\n",
    "f_train = np.concatenate([frame.arrays['forces'] for frame in frames_train])\n",
    "# Test on the remaining 200 molecules\n",
    "test_ids = [int(i) for i in ids[n_train:]] \n",
    "frames_test = [frames[ii] for ii in test_ids]\n",
    "e_test = np.array([energies[ii] for ii in test_ids])\n",
    "f_test = np.concatenate([frame.arrays['forces'] for frame in frames_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b367a",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic energy baseline - assuming all configurations have the same number of atoms\n",
    "atom_energy_baseline = np.mean(energies)/len(frames[0])\n",
    "energy_baseline = {int(species): atom_energy_baseline for species in global_species}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41ac68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameters of the spherical expansion\n",
    "hypers = dict(soap_type=\"PowerSpectrum\",\n",
    "              interaction_cutoff=3.0, \n",
    "              max_radial=8, \n",
    "              max_angular=6, \n",
    "              gaussian_sigma_constant=0.5,\n",
    "              gaussian_sigma_type=\"Constant\",\n",
    "              cutoff_function_type=\"RadialScaling\",\n",
    "              cutoff_smooth_width=0.5,\n",
    "              cutoff_function_parameters=\n",
    "                    dict(\n",
    "                            rate=1,\n",
    "                            scale=3.5,\n",
    "                            exponent=4\n",
    "                        ),\n",
    "              radial_basis=\"GTO\",\n",
    "              normalize=True,\n",
    "              optimization=\n",
    "                    dict(\n",
    "                            Spline=dict(\n",
    "                               accuracy=1.0e-05\n",
    "                            )\n",
    "                        ),\n",
    "              compute_gradients=False\n",
    "              )\n",
    "\n",
    "\n",
    "soap = SphericalInvariants(**hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b243f",
   "metadata": {},
   "source": [
    "## Compute descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "managers = []\n",
    "for f in frames_train:\n",
    "    f.wrap(eps=1e-18)\n",
    "\n",
    "start = time()\n",
    "managers = soap.transform(tqdm(frames_train))\n",
    "print (\"Execution: \", time()-start, \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2732fd",
   "metadata": {},
   "source": [
    "## Sample selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2c3d5",
   "metadata": {},
   "source": [
    "Now we select the set of sparse _samples_ (environments).  In effect, we are selecting an optimally diverse set of _rows_ of the feature matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5982b",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# select the sparse points for the sparse kernel method with FPS on the whole training set\n",
    "n_sparse_env = {1:50, 8:100}\n",
    "sample_compressor = FPSFilter(soap, n_sparse_env, act_on='sample per species')\n",
    "X_sparse = sample_compressor.select_and_filter(managers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e254859",
   "metadata": {},
   "source": [
    "## Model assessment: Sparse samples only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e67152",
   "metadata": {},
   "source": [
    "Let's build our potential and get some benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = 2\n",
    "\n",
    "start = time()\n",
    "hypers['compute_gradients'] = True\n",
    "soap_grads = SphericalInvariants(**hypers)\n",
    "kernel = Kernel(soap_grads, name='GAP', zeta=zeta, target_type='Structure', kernel_type='Sparse')\n",
    "\n",
    "KNM = compute_KNM(tqdm(frames_train, leave=True, desc=\"Computing kernel matrix\"), X_sparse, kernel, soap_grads)\n",
    "\n",
    "model = train_gap_model(kernel, frames_train, KNM, X_sparse, e_train, energy_baseline, \n",
    "                        grad_train=-f_train, lambdas=[1e-12, 1e-12], jitter=1e-13)\n",
    "\n",
    "# save the model to a file in json format for future use\n",
    "dump_obj('zundel_model.json', model)\n",
    "np.savetxt('Structure_indices.txt', ids)\n",
    "print (\"Execution: \", time()-start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "e_pred = []\n",
    "f_pred = []\n",
    "for f in tqdm(frames_test):\n",
    "    positions = f.get_positions()\n",
    "    f.wrap(eps=1e-18)\n",
    "    m = soap_grads.transform(f)\n",
    "    e_pred.append(model.predict(m)[0])\n",
    "    f_pred.append(model.predict_forces(m))\n",
    "\n",
    "e_pred = np.array(e_pred)\n",
    "f_pred = np.concatenate(f_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065ae1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascal.utils import get_score\n",
    "\n",
    "score = get_score(e_pred, e_test)\n",
    "RMSE = score['RMSE']\n",
    "sigma_test = np.std(e_test)\n",
    "print(\"RMSE = \", RMSE*1000.0, \"meV\")\n",
    "print(\"Test set stdev = \", sigma_test, \" eV\")\n",
    "print(\"Relative RMSE = \", RMSE/sigma_test*100.0, \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa07ef",
   "metadata": {},
   "source": [
    "## Now introduce feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecb1e3d",
   "metadata": {},
   "source": [
    "This potential uses quite a large number of features to make its predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soap.get_num_coefficients(n_species=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761f4b0",
   "metadata": {},
   "source": [
    "Since the cost to compute the kernel (and hence evaluate the model) scales asymptotically linearly with the number of features [2], it would be nice if we could get away with using fewer components of the feature vector.\n",
    "\n",
    "[2]: F. Musil, M. Veit, A. Goscinski, G. Fraux, M. J. Willatt, M. Stricker, T. Junge, and M. Ceriotti, Efficient Implementation of Atom-Density Representations, J. Chem. Phys. 154, 114109 (2021). [doi:10.1063/5.0044689](https://aip.scitation.org/doi/10.1063/5.0044689)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2381cf3",
   "metadata": {},
   "source": [
    "To select features (columns) instead of samples (rows), we use the same selector class as above with a few changes.  Let's start with a conservative estimate of half the total number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc20224",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# select the features with FPS, again the whole training set\n",
    "n_sparse_feat = 600\n",
    "feat_compressor = FPSFilter(soap, n_sparse_feat, act_on='feature')\n",
    "feat_sparse_parameters = feat_compressor.select_and_filter(managers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c54d51",
   "metadata": {},
   "source": [
    "Note that here we use the full feature matrix again, but in real scientific applications this might not be possible or practical due to the size of the full matrix.  In this case we might sub-select samples, either randomly or using our existing sparse sample selection*, to obtain a smaller matrix to work with.\n",
    "\n",
    "*You might be wondering how we do the sparse sample selection in the first place if the full feature matrix is too big to fit in memory.  There are various ways around this, including iterative or hierarchical selection methods, or we could just do feature selection first on a random subset and then do sample selection on the feature-sparsified matrix.  The best method to use will in general depend on the application and is out of the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55140518",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sparse_parameters.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d74eb07",
   "metadata": {},
   "source": [
    "The result is a dictionary that tells the `SphericalInvariants` class which coefficents to compute.  In order to use it, we must update the `hypers` and create a new, sparsified version of the `soap` feature calculator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers['coefficient_subselection'] = feat_sparse_parameters['coefficient_subselection']\n",
    "soap_fsparse = SphericalInvariants(**hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d21b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "managers_sparsefeat = soap_fsparse.transform(tqdm(frames_train))\n",
    "print(\"Execution time: {} s\".format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2119cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bit of a hack because we can't directly use the old `sample_compressor` on the new features\n",
    "sample_compressor_sparse = FPSFilter(soap_fsparse, n_sparse_env, act_on='sample per species')\n",
    "sample_compressor_sparse.selected_sample_ids_by_sp = sample_compressor.selected_sample_ids_by_sp\n",
    "X_sparse_sparsefeat = sample_compressor_sparse.filter(managers_sparsefeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff3c7e",
   "metadata": {},
   "source": [
    "### How do we know how many features to select?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f13fe5d",
   "metadata": {},
   "source": [
    "With FPS, one metric we can look at to get a rough estimate of the required number of features is the Hausdorff distance of each selected point (basically, the distance of the selected point to the closest point in the \"blob\" of already-selected points).  This tells us how far (in feature space) each selected point is from the selected set, and _very_ roughly, the amount of diversity it adds to the set.  So when the decrease in this distance starts to slow down, it indicates that we are getting less of an information return for each point added to the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_dists = feat_compressor.get_fps_distances()\n",
    "plt.semilogy(fps_dists)\n",
    "plt.xlabel('Selected feature index')\n",
    "plt.ylabel('Hausdorff distance')\n",
    "plt.title('FPS selection on features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9b6430",
   "metadata": {},
   "source": [
    "From this plot, it seems that we might even be able to get away with 200 or 300 features, but we should test this more rigorously.  What we really care about is how well these sparsified features reproduce the _kernel_ -- so let's look at the difference in the kernel between the sparse _environments_, with and without _feature_ sparsification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_MM_fullfeat = kernel(X_sparse)\n",
    "kernel_sparsefeat = Kernel(soap_fsparse, name='GAP', zeta=zeta, target_type='Structure', kernel_type='Sparse')\n",
    "K_MM_sparsefeat = kernel_sparsefeat(X_sparse_sparsefeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(K_MM_fullfeat - K_MM_sparsefeat, 'fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(K_MM_fullfeat, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca20ac8",
   "metadata": {},
   "source": [
    "So the difference in the norms is less than 1% of the norm of the kernel matrix.  Let's see if we can use an even smaller number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_sparsified_soap(Nselect, full_soap, managers, sample_compressor):\n",
    "    feat_compressor = FPSFilter(full_soap, Nselect, act_on='feature')\n",
    "    feat_sparse_parameters = feat_compressor.select_and_filter(managers)\n",
    "    soap_hypers = full_soap._get_init_params()\n",
    "    soap_hypers['coefficient_subselection'] = feat_sparse_parameters['coefficient_subselection']\n",
    "    soap_fsparse = SphericalInvariants(**soap_hypers)\n",
    "    managers_sparsefeat = soap_fsparse.transform(managers)\n",
    "    sample_compressor_sparse = FPSFilter(soap_fsparse, n_sparse_env, act_on='sample per species')\n",
    "    sample_compressor_sparse.selected_sample_ids_by_sp = sample_compressor.selected_sample_ids_by_sp\n",
    "    X_sparse_sparsefeat = sample_compressor_sparse.filter(managers_sparsefeat)\n",
    "    return soap_fsparse, managers_sparsefeat, X_sparse_sparsefeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab26edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soap_smallsparse, managers_sparsefeat, X_sparse_sparsefeat = get_feature_sparsified_soap(\n",
    "    200, soap, managers, sample_compressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sparsefeat = Kernel(soap_smallsparse, name='GAP', zeta=zeta, target_type='Structure', kernel_type='Sparse')\n",
    "K_MM_sparsefeat = kernel_sparsefeat(X_sparse_sparsefeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf5422",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(K_MM_fullfeat - K_MM_sparsefeat, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397853b",
   "metadata": {},
   "source": [
    "Barely a difference. So let's proceed with 200 features and see how accurate our model is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca42c50",
   "metadata": {},
   "source": [
    "## Model assessment: Sparse samples and sparse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta = 2\n",
    "\n",
    "start = time()\n",
    "sparse_hypers = soap_smallsparse._get_init_params()\n",
    "sparse_hypers['compute_gradients'] = True\n",
    "soap_fsparse_grads = SphericalInvariants(**sparse_hypers)\n",
    "kernel_fsparse = Kernel(soap_fsparse_grads, name='GAP', zeta=zeta, target_type='Structure', kernel_type='Sparse')\n",
    "\n",
    "KNM = compute_KNM(tqdm(frames_train, leave=True, desc=\"Computing kernel matrix\"), X_sparse_sparsefeat, kernel_fsparse, soap_fsparse_grads)\n",
    "\n",
    "model_fsparse = train_gap_model(\n",
    "    kernel_fsparse,\n",
    "    frames_train,\n",
    "    KNM,\n",
    "    X_sparse_sparsefeat,\n",
    "    e_train,\n",
    "    energy_baseline,\n",
    "    grad_train=-f_train,\n",
    "    lambdas=[1e-12, 1e-12],\n",
    "    jitter=1e-13\n",
    ")\n",
    "\n",
    "# save the model to a file in json format for future use\n",
    "dump_obj('zundel_model_featsparse.json', model)\n",
    "np.savetxt('Structure_indices.txt', ids)\n",
    "print (\"Execution: \", time()-start, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d31fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "e_pred = []\n",
    "f_pred = []\n",
    "for f in tqdm(frames_test):\n",
    "    positions = f.get_positions()\n",
    "    f.wrap(eps=1e-18)\n",
    "    m = soap_fsparse_grads.transform(f)\n",
    "    e_pred.append(model_fsparse.predict(m))\n",
    "    f_pred.append(model_fsparse.predict_forces(m))\n",
    "\n",
    "e_pred = np.array(e_pred)\n",
    "f_pred = np.concatenate(f_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b320a45b",
   "metadata": {},
   "source": [
    "Notice how it is already much faster to train and evaluate a model.  Now, how accurate is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff52672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascal.utils import get_score\n",
    "\n",
    "score = get_score(e_pred.flat, e_test)\n",
    "RMSE = score['RMSE']\n",
    "sigma_test = np.std(e_test)\n",
    "print(\"RMSE = \", RMSE*1000.0, \"meV\")\n",
    "print(\"Test set stdev = \", sigma_test, \" eV\")\n",
    "print(\"Relative RMSE = \", RMSE/sigma_test*100.0, \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396f60c",
   "metadata": {},
   "source": [
    "In fact, it's slightly _more_ accurate than the model built with the full feature set.  This might be because the model built with the full features overfits slightly, since it has more than enough features to describe the dataset.  If we were to optimize the regularizers for both these fits, it is likely that the feature-sparsified model would have approximately equal or lower accuracy than the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03fdbc8",
   "metadata": {},
   "source": [
    "## Timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c9c77",
   "metadata": {},
   "source": [
    "Now, back to our main motivation for doing feature sparsification.  If we use these models to run molecular dynamics (MD), how much of a speedup do we get with the feature-sparsified model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.md import MDLogger\n",
    "from ase.md.langevin import Langevin\n",
    "from ase import units\n",
    "from ase.io.trajectory import Trajectory\n",
    "from ase.md.velocitydistribution import MaxwellBoltzmannDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rascal.models.asemd import ASEMLCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3ee9d",
   "metadata": {},
   "source": [
    "Let's use the ASE calculator (as in `MLIP_example.ipynb`), which we can run directly in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d996ef",
   "metadata": {},
   "source": [
    "Full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29218ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = frames_test[0].copy()\n",
    "calc = ASEMLCalculator(model, soap_grads)\n",
    "T = 200\n",
    "\n",
    "start = time()\n",
    "\n",
    "MaxwellBoltzmannDistribution(atoms, T * units.kB)\n",
    "atoms.set_calculator(calc)\n",
    "dyn = Langevin(atoms, 0.5 * units.fs, units.kB * T, 0.002)\n",
    "dyn.run(500)\n",
    "\n",
    "elapsed = time() - start\n",
    "print(\"Elapsed time: {} s ({} ms per timestep)\".format(elapsed, elapsed / 500 * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e48a5b",
   "metadata": {},
   "source": [
    "Feature-sparsified model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afd338",
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = frames_test[0].copy()\n",
    "calc = ASEMLCalculator(model_fsparse, soap_fsparse_grads)\n",
    "T = 200\n",
    "\n",
    "start = time()\n",
    "\n",
    "MaxwellBoltzmannDistribution(atoms, T * units.kB)\n",
    "atoms.set_calculator(calc)\n",
    "dyn = Langevin(atoms, 0.5 * units.fs, units.kB * T, 0.002)\n",
    "dyn.run(500)\n",
    "\n",
    "elapsed = time() - start\n",
    "print(\"Elapsed time: {} s ({} ms per timestep)\".format(elapsed, elapsed / 500 * 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80dfa8",
   "metadata": {},
   "source": [
    "So our feature-sparsified model is about twice as fast as the full model, and with the same accuracy!\n",
    "\n",
    "(But since we selected about one-sixth of the features, why isn't it 6x as fast? Well, there are various overheads, especially in the computation of gradients that are needed for MD forces, that keep the computational cost from scaling exactly linearly with the feature vector size.  We could probably push the cost down even further by optimizing the feature size more aggressively, but if we want to keep the same accuracy we probably won't be able to get a speedup above about 3x.  Note also that the impact of this optimization is much more pronounced for models with much larger feature sizes -- e.g. many atomic species or high `max_radial`/`max_angular` parameters.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
